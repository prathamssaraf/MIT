{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Exercise 1 -Create a  environment \n",
    "\n",
    "a.  for  which  the  observation  is  a  random  integer  between -5  and  5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CustomEnv:\n",
    "    def __init__(self):\n",
    "        self.observation_space = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "        self.action_space = [0, 1, 2]\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = random.choice(self.observation_space)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert action in self.action_space\n",
    "        reward = action * self.state\n",
    "        done = True\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(observation):\n",
    "    if observation < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Request  for  50  observations  from  the  environment,  compute  and  print  the total reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 92\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(50):\n",
    "    observation = env.reset()\n",
    "    action = optimal_policy(observation)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward:\", total_reward)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 â€“Create an environment \n",
    "\n",
    "a.Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CustomEnv:\n",
    "    def __init__(self, flipped=False):\n",
    "        self.observation_space = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "        self.action_space = [0, 1, 2]\n",
    "        self.state = None\n",
    "        self.flipped = flipped\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = random.choice(self.observation_space)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert action in self.action_space\n",
    "        if self.flipped:\n",
    "            reward = -action * self.state\n",
    "        else:\n",
    "            reward = action * self.state\n",
    "        done = True\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Define a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle:\n",
    "\n",
    "i.The agent has not detected know yet which version of the environment is running.\n",
    "\n",
    "ii.The  agent  detected  that  the  original  version  of  the  environment  is running.\n",
    "\n",
    "iii.The  agent  detected  that  the  flipped  version  of  the  environment  is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self):\n",
    "        self.detected = False\n",
    "        self.flipped = False\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if self.detected:\n",
    "            if self.flipped:\n",
    "                return 2 if observation < 0 else 0\n",
    "            else:\n",
    "                return 0 if observation < 0 else 2\n",
    "        else:\n",
    "            if observation == 0:\n",
    "                return 1\n",
    "            elif observation < 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 2\n",
    "            \n",
    "    def update(self, observation, action, reward):\n",
    "        if not self.detected:\n",
    "            if reward == action * observation:\n",
    "                self.detected = True\n",
    "            elif reward == -action * observation:\n",
    "                self.detected = True\n",
    "                self.flipped = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Define the agent that detects the sign of the environment and sets the policy appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = CustomEnv()\n",
    "        self.policy = Policy()\n",
    "        \n",
    "    def run_episode(self):\n",
    "        observation = self.env.reset()\n",
    "        action = self.policy.choose_action(observation)\n",
    "        state, reward, done, _ = self.env.step(action)\n",
    "        self.policy.update(observation, action, reward)\n",
    "        return reward\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        total_reward = 0\n",
    "        for i in range(num_episodes):\n",
    "            reward = self.run_episode()\n",
    "            total_reward += reward\n",
    "        return total_reward\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
